{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Numerical experiments - Toxic comments.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyM+tcy6YmH2Ztoukz/cPcuG",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaeyoung-jae-park/Joint-model-assisted-Decision-Rule/blob/main/Numerical_experiments_Toxic_comments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMLxzsIkfBD4"
      },
      "source": [
        "# Data preparation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iShTazJ6pSv9"
      },
      "source": [
        "## Data Loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kr5K2JaAmbZr"
      },
      "source": [
        "There are two options to download the dataset of Toxic comments: 1) using Kaggle API; or 2) loading it from your local drive. We encourage you to choose Option 1 because Option 2 takes longer time than the other.\n",
        "\n",
        "If you want to use Kaggle API, you should create new API token. The following document explains how to create it: https://galhever.medium.com/how-to-import-data-from-kaggle-to-google-colab-8160caa11e2.\n",
        "\n",
        "If you want to load it from your local drive, download the dataset here: https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data\n",
        "\n",
        "We only need train.csv for this experiment.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cG-ER4cemy-n"
      },
      "source": [
        "### Using Kaggle API\n",
        "Upload your Kaggle API Token and then run the following coddes. \n",
        "\n",
        "Procedure\n",
        "1.   Run the next block.\n",
        "2.   Click the button of \"Choose Files.\"\n",
        "3.   Upload kaggle.json.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "B3HQBYuf7pX8",
        "outputId": "5932e394-1677-4a66-e728-38e72de96ed2"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-880f9dc3-f81d-4c0c-a283-62af5b327eaf\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-880f9dc3-f81d-4c0c-a283-62af5b327eaf\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"jaeyoungpark\",\"key\":\"0560c108cef0bdd5c48cc2c16db24253\"}'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8TF8kjL7vKs"
      },
      "source": [
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inhPPkFl7w40"
      },
      "source": [
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rv9RECes7ySV",
        "outputId": "dd3e92c7-1271-41a8-88c8-84c179f363be"
      },
      "source": [
        "!kaggle competitions download -c jigsaw-toxic-comment-classification-challenge"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4)\n",
            "Downloading train.csv.zip to /content\n",
            " 19% 5.00M/26.3M [00:00<00:02, 9.29MB/s]\n",
            "100% 26.3M/26.3M [00:00<00:00, 41.5MB/s]\n",
            "Downloading test.csv.zip to /content\n",
            " 38% 9.00M/23.4M [00:00<00:00, 16.2MB/s]\n",
            "100% 23.4M/23.4M [00:00<00:00, 34.4MB/s]\n",
            "Downloading test_labels.csv.zip to /content\n",
            "  0% 0.00/1.46M [00:00<?, ?B/s]\n",
            "100% 1.46M/1.46M [00:00<00:00, 195MB/s]\n",
            "Downloading sample_submission.csv.zip to /content\n",
            "  0% 0.00/1.39M [00:00<?, ?B/s]\n",
            "100% 1.39M/1.39M [00:00<00:00, 94.4MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhH_xojS74Do",
        "outputId": "e661590f-2087-42a7-b661-c45b6aa33e2d"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "kaggle.json  sample_submission.csv.zip\ttest_labels.csv.zip\n",
            "sample_data  test.csv.zip\t\ttrain.csv.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TN0vBYmA765j",
        "outputId": "52e8f487-aa1b-4402-eb97-a21ea7c735b0"
      },
      "source": [
        "!mkdir toxic\n",
        "!unzip sample_submission.csv.zip -d toxic\n",
        "!unzip test_labels.csv.zip -d toxic\n",
        "!unzip test.csv.zip -d toxic\n",
        "!unzip train.csv.zip -d toxic"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  sample_submission.csv.zip\n",
            "  inflating: toxic/sample_submission.csv  \n",
            "Archive:  test_labels.csv.zip\n",
            "  inflating: toxic/test_labels.csv   \n",
            "Archive:  test.csv.zip\n",
            "  inflating: toxic/test.csv          \n",
            "Archive:  train.csv.zip\n",
            "  inflating: toxic/train.csv         \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2bsZX7c8NKM"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "train = pd.read_csv(\"/content/toxic/train.csv\") # feel free to change the location"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SaitexpMm6DD"
      },
      "source": [
        "### Loading the dataset from your local drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhg0PG3Gm4pl"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_uP0cb9n3zB"
      },
      "source": [
        "import io\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "train = pd.read_csv(io.BytesIO(uploaded['train.csv']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ssf5RhT_211"
      },
      "source": [
        "### Checking to load the dataset correctly"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PybuLU1oTbM"
      },
      "source": [
        "The codes below show the distribution for each label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FrNvliWl3fJA",
        "outputId": "d7778946-75aa-413c-e017-d4945e5ade62"
      },
      "source": [
        "np.hstack((train['toxic'].value_counts()[1], \n",
        "           train['severe_toxic'].value_counts()[1], \n",
        "           train['obscene'].value_counts()[1],\n",
        "           train['threat'].value_counts()[1],\n",
        "           train['insult'].value_counts()[1],\n",
        "           train['identity_hate'].value_counts()[1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([15294,  1595,  8449,   478,  7877,  1405])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tw3BeiXx8nJr",
        "outputId": "e954d056-fcf9-4ec4-fc63-6ca2242387f8"
      },
      "source": [
        "train['toxic'].value_counts()/np.sum(train['toxic'].value_counts())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    0.904156\n",
              "1    0.095844\n",
              "Name: toxic, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODTWLJyH8zAP",
        "outputId": "facc01d1-0c88-4c8b-c379-80791de88365"
      },
      "source": [
        "train['severe_toxic'].value_counts()/np.sum(train['severe_toxic'].value_counts())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    0.990004\n",
              "1    0.009996\n",
              "Name: severe_toxic, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jq06pK3q83sa",
        "outputId": "cf847e11-e3a5-4403-d4b9-0fd5466f5020"
      },
      "source": [
        "train['obscene'].value_counts()/np.sum(train['obscene'].value_counts())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    0.947052\n",
              "1    0.052948\n",
              "Name: obscene, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYZ6vlyO86cB",
        "outputId": "c6578ade-5995-4c43-ec34-ab042ef62a22"
      },
      "source": [
        "train['threat'].value_counts()/np.sum(train['threat'].value_counts())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    0.997004\n",
              "1    0.002996\n",
              "Name: threat, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rCPe3nSi8-Ob",
        "outputId": "184bb0c3-dd50-4ccc-bed7-14f3ee7707dd"
      },
      "source": [
        "train['insult'].value_counts()/np.sum(train['insult'].value_counts())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    0.950636\n",
              "1    0.049364\n",
              "Name: insult, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JwCvhC2z9BeB",
        "outputId": "249183fe-90b2-4db9-f681-dd89297f3568"
      },
      "source": [
        "train['identity_hate'].value_counts()/np.sum(train['identity_hate'].value_counts())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    0.991195\n",
              "1    0.008805\n",
              "Name: identity_hate, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "fQd3khaTgYkF",
        "outputId": "28637d66-8f77-41e6-aa8b-2e67a6f0d644"
      },
      "source": [
        "pd.crosstab(train['toxic'], train['obscene'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>obscene</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>toxic</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>143754</td>\n",
              "      <td>523</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>7368</td>\n",
              "      <td>7926</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "obscene       0     1\n",
              "toxic                \n",
              "0        143754   523\n",
              "1          7368  7926"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "qqcONSBGgl2n",
        "outputId": "bc002799-821e-4a5b-fde9-e8d42b3fd568"
      },
      "source": [
        "pd.crosstab(train['toxic'], train['insult'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>insult</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>toxic</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>143744</td>\n",
              "      <td>533</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>7950</td>\n",
              "      <td>7344</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "insult       0     1\n",
              "toxic               \n",
              "0       143744   533\n",
              "1         7950  7344"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFgarPJdonOS"
      },
      "source": [
        "## Label setting\n",
        "Target - toxic comments\n",
        "\n",
        "Auxiliary outcomes - obscene and insult"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-opl1PE_ux8"
      },
      "source": [
        "X = train['comment_text']\n",
        "y = train[['toxic','severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']]\n",
        "y_aux = train[['toxic', 'obscene', 'insult']]\n",
        "y_aux_agg = pd.concat([y_aux, y_aux.iloc[:,0] * y_aux.iloc[:,1],\n",
        "                        y_aux.iloc[:,0] * y_aux.iloc[:,2]], axis=1)\n",
        "y_tgt = train['toxic']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "16MvhTWOaBEU",
        "outputId": "e0a17659-cb5a-4aba-db4b-a8c4f2a7ba5d"
      },
      "source": [
        "y_aux_agg"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>toxic</th>\n",
              "      <th>obscene</th>\n",
              "      <th>insult</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159566</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159567</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159568</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159569</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159570</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>159571 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        toxic  obscene  insult  0  1\n",
              "0           0        0       0  0  0\n",
              "1           0        0       0  0  0\n",
              "2           0        0       0  0  0\n",
              "3           0        0       0  0  0\n",
              "4           0        0       0  0  0\n",
              "...       ...      ...     ... .. ..\n",
              "159566      0        0       0  0  0\n",
              "159567      0        0       0  0  0\n",
              "159568      0        0       0  0  0\n",
              "159569      0        0       0  0  0\n",
              "159570      0        0       0  0  0\n",
              "\n",
              "[159571 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjNN575Hpd_w"
      },
      "source": [
        "# Hyperparameter settings\n",
        "\n",
        "Neural network structures are defined for the following seven models: Baseline, NN, NN-joint, LDR-NN-joint, LDR-CIDNN, NLDR-NN-joint, and NLDR-CIDNN.\n",
        "\n",
        "The optimizer is stochastic gradient descent with a learning rate of $10^{-3}$ and a momentum of 0.9. \n",
        "For each training set (9 folds), we further split it and use 20\\% of the data as the validation set to avoid overfitting. \n",
        "The learning rate will decrease and the training may stop early based on the validation loss values. \n",
        "When a validation loss value does not achieve one smaller than the minimum of the last 5 epochs, the learning rate reduces to one tenth of its previous value. Further, although the total number of epochs is 300, the training will stop early, if a loss value smaller than the minimum is not obtained within 10 epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkrz1HHBpswY"
      },
      "source": [
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from keras.optimizers import SGD\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcL4_JBWxlcU"
      },
      "source": [
        "## Baseline\n",
        "\n",
        "A linear decision rule does not have hidden layers. The network only consists of an input layer and an output layer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ng5j7WPGXaAV"
      },
      "source": [
        "def define_Baseline(output_dim):\n",
        "\n",
        "  output_dim = output_dim\n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(Dense(output_dim, input_shape=(vocab_size,), activation = \"sigmoid\"))\n",
        "  model.summary()\n",
        "  return model\n",
        "\n",
        "def train_Baseline(model, X, y, epochs, batch_size):\n",
        "  if model is None:\n",
        "    if len(y.shape) == 1:\n",
        "      model = define_Baseline(1)\n",
        "    else: model= define_Baseline(y.shape[1])\n",
        "  es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
        "  rl = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5)\n",
        "\n",
        "  model.compile(loss='binary_crossentropy', optimizer=SGD(learning_rate = 0.001, momentum=0.9), metrics=['accuracy'])\n",
        "  history = model.fit(X , y, batch_size = batch_size, epochs = epochs, validation_split=0.2, callbacks=[es, rl], verbose = 2)\n",
        "  return model, history\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZa97Umvq-8y"
      },
      "source": [
        "## NN and NN-joint\n",
        "\n",
        "The network includes two fully-connected hidden layers, with 256 and 128 units, respectively, and an additional dropout layer with a drop rate of 0.1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRCZdF-WhLsO"
      },
      "source": [
        "def define_NN(output_dim):\n",
        "\n",
        "  output_dim = output_dim\n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(Dense(256, input_shape=(vocab_size,), activation='relu'))\n",
        "  model.add(Dense(128, activation='relu'))\n",
        "  model.add(Dropout(0.1))\n",
        "  model.add(Dense(output_dim, activation = \"sigmoid\"))\n",
        "  model.summary()\n",
        "  return model\n",
        "\n",
        "def train_NN(model, X, y, epochs, batch_size):\n",
        "  if model is None:\n",
        "    if len(y.shape) == 1:\n",
        "      model = define_NN(1)\n",
        "    else: model= define_NN(y.shape[1])\n",
        "  es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
        "  rl = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5)\n",
        "  model.compile(loss='binary_crossentropy', optimizer=SGD(learning_rate = 0.001, momentum=0.9), metrics=['accuracy'])\n",
        "  history = model.fit(X , y, batch_size = batch_size, epochs = epochs, validation_split=0.2, callbacks=[es, rl], verbose=2)\n",
        "  return model, history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7xR0zRMx7iz"
      },
      "source": [
        "## CIDNN\n",
        "\n",
        "To calculate CIScore correctly, the indicies should match the the positions of auxiliary outcomes. The output layer and loss function for CIDNN is different from those for NN, whereas the input layer and hidden layers of CIDNN are identical."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrPzER6Kx4dT"
      },
      "source": [
        "def CIScore(y_true, y_pred):\n",
        "  CI_numer = []\n",
        "  CI_numer.append(tf.square((y_pred[:, 3] - y_pred[:, 1] * y_pred[:, 0])))\n",
        "  CI_numer.append(tf.square((y_pred[:, 4] - y_pred[:, 2] * y_pred[:, 0])))\n",
        "  \n",
        "  return tf.reduce_mean(CI_numer[0], axis=-1)/CI_denom[0] + tf.reduce_mean(CI_numer[1], axis=-1)/CI_denom[1] + tf.keras.losses.binary_crossentropy(y_true[:,0], y_pred[:,0])\n",
        "\n",
        "def train_CIDNN(model, X, y, epochs, batch_size):\n",
        "  if model is None:\n",
        "    model = define_NN(y.shape[1])\n",
        "\n",
        "  es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
        "  rl = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5)\n",
        "\n",
        "  model.compile(loss=CIScore, optimizer=SGD(learning_rate = 0.001, momentum=0.9), metrics=['accuracy'])\n",
        "  history = model.fit(X , y, batch_size = batch_size, epochs = epochs, validation_split=0.2, callbacks=[es, rl], verbose=2)\n",
        "  return model, history\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9BmSfd-tzId"
      },
      "source": [
        "## Joint-model-assisted linear/nonlinear decision rules (LDR/NLDR)\n",
        "\n",
        "The original X and transformed X are concatenated for the input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXRY3xI6y1tG"
      },
      "source": [
        "def define_LDR(input_shape):\n",
        "\n",
        "  output_dim = 1\n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(Dense(output_dim, input_dim = input_shape, activation = \"sigmoid\"))\n",
        "  model.summary()\n",
        "  return model\n",
        "\n",
        "def train_LDR(model, X, X_extracted, y, epochs, batch_size):\n",
        "  X_transfered = np.hstack((X, X_extracted))\n",
        "\n",
        "  if model is None:\n",
        "    model = define_LDR(X_transfered.shape[1])\n",
        "\n",
        "  es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
        "  rl = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5)\n",
        "\n",
        "  model.compile(loss='binary_crossentropy', optimizer=SGD(learning_rate = 0.001, momentum=0.9), metrics=['accuracy'])\n",
        "  history = model.fit(X_transfered , y, batch_size = batch_size, epochs = epochs, validation_split=0.2, callbacks=[es,rl], verbose = 2)\n",
        "  return model, history\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbJKpkCjt76J"
      },
      "source": [
        "def define_NLDR(input_shape):\n",
        "\n",
        "  output_dim = 1\n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(Dense(256, input_shape=(input_shape,), activation='relu'))\n",
        "  model.add(Dense(128, activation='relu'))\n",
        "  model.add(Dropout(0.1))\n",
        "  model.add(Dense(output_dim, activation = \"sigmoid\"))\n",
        "  model.summary()\n",
        "  return model\n",
        "\n",
        "def train_NLDR(model, X, X_extracted, y, epochs, batch_size):\n",
        "  X_transfered = np.hstack((X, X_extracted))\n",
        "\n",
        "  if model is None:\n",
        "    model = define_NLDR(X_transfered.shape[1])\n",
        "\n",
        "  es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
        "  rl = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5)\n",
        "\n",
        "  model.compile(loss='binary_crossentropy', optimizer=SGD(learning_rate = 0.001, momentum=0.9), metrics=['accuracy'])\n",
        "  history = model.fit(X_transfered , y, batch_size = batch_size, epochs = epochs, validation_split=0.2, callbacks=[es, rl], verbose = 2)\n",
        "  return model, history\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TverzfRaX-Ju"
      },
      "source": [
        "# Training\n",
        "\n",
        "We use 10-fold cross-validation for the overall procedure. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1QKVGXXX9Uo"
      },
      "source": [
        "X = np.array(X)\n",
        "y_aux_agg = np.array(y_aux_agg)\n",
        "y_tgt = np.array(y_tgt)\n",
        "y_aux = np.array(y_aux)\n",
        "np.random.seed(42)\n",
        "fold_idx = np.random.choice(np.hstack((np.repeat(np.arange(10), int(X.shape[0]/10)), np.arange(X.shape[0] % 10))), size=X.shape[0], replace=False)\n",
        "\n",
        "def split_data(X, y, fold_no):\n",
        "  X_train, X_test, y_train, y_test = X[fold_idx != fold_no], X[fold_idx == fold_no], y[fold_idx != fold_no], y[fold_idx == fold_no]\n",
        "  np.random.seed(fold_no)\n",
        "  shuffle_train = np.random.choice(X_train.shape[0], X_train.shape[0], replace=False)\n",
        "  shuffle_test = np.random.choice(X_test.shape[0], X_test.shape[0], replace=False)\n",
        "  return X_train[shuffle_train], X_test[shuffle_test], y_train[shuffle_train], y_test[shuffle_test]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ka7lS0dffK1H"
      },
      "source": [
        "We strongly encourage you to mount Google drive with the given directory, '/content/drive' to reproduce our work easily. Also, it takes long time to save and load models from your local drive. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4ItzJJs7fZh",
        "outputId": "a080f3ab-4fb9-4a4d-b36a-9ede199be6f2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNaNiKo_k-bx"
      },
      "source": [
        "You might have a lack of RAM since the size of the dataset is large. Then, run the code regarding the training separately. Use the alternative code below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCoaGIEEYJQM"
      },
      "source": [
        "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
        "from keras.models import Model\n",
        "import keras\n",
        "\n",
        "for fold_no in np.arange(10):\n",
        "  X_train, X_test, y_aux1_train, y_aux1_test = split_data(X, y_aux_agg, fold_no)\n",
        "  X_train, X_test, y_tgt_train, y_tgt_test = split_data(X, y_tgt, fold_no)\n",
        "  y_aux1_train = np.array(y_aux1_train, dtype='float32')\n",
        "\n",
        "  # Create a tf-idf matrix\n",
        "  vocab_size = 5000\n",
        "\n",
        "  tokenizer_obj = Tokenizer(num_words = vocab_size)\n",
        "  tokenizer_obj.fit_on_texts(X_train)\n",
        "\n",
        "  max_length = max([len(s.split()) for s in X])\n",
        "\n",
        "  X_train_tokens = tokenizer_obj.texts_to_matrix(X_train, mode='tfidf')\n",
        "  X_test_tokens = tokenizer_obj.texts_to_matrix(X_test, mode='tfidf')\n",
        "\n",
        "  # Baseline\n",
        "  model_Baseline, history_Baseline = train_Baseline(model = None, X= X_train_tokens, y = y_tgt_train, epochs = 300, batch_size = 256)\n",
        "  model_Baseline.save(\"/content/drive/MyDrive/Can_a_joint_model_assist_target_label_prediction/Toxic_comments/model_Baseline_\"+str(fold_no))\n",
        "  del model_Baseline\n",
        "  keras.backend.clear_session()\n",
        "  \n",
        "  # NN\n",
        "  model_NN, history_NN = train_NN(model = None, X= X_train_tokens, y = y_tgt_train, epochs = 300, batch_size = 256)\n",
        "  model_NN.save(\"/content/drive/MyDrive/Can_a_joint_model_assist_target_label_prediction/Toxic_comments/model_NN_\"+str(fold_no))\n",
        "  del model_NN\n",
        "  keras.backend.clear_session()\n",
        "\n",
        "  # NN-joint\n",
        "  model_NN_joint, history_NN_joint = train_NN(model = None, X= X_train_tokens, y = y_aux1_train[:,0:3], epochs = 300, batch_size = 256)\n",
        "  model_NN_joint.save(\"/content/drive/MyDrive/Can_a_joint_model_assist_target_label_prediction/Toxic_comments/model_NN_joint_\"+str(fold_no))\n",
        "  del model_NN_joint\n",
        "  keras.backend.clear_session()\n",
        "\n",
        "\n",
        "  # CIDNN\n",
        "\n",
        "  ## the denominator of the first term of CIScore\n",
        "  sample_size = y_aux1_train.shape[0]\n",
        "  CI_denom = []\n",
        "  CI_denom.append((tf.square(tf.reduce_sum(y_aux1_train[:,3], axis=-1)/sample_size - \n",
        "                          tf.reduce_sum(y_aux1_train[:,1], axis=-1)/sample_size * \n",
        "                          tf.reduce_sum(y_aux1_train[:,0], axis=-1)/sample_size)))\n",
        "  CI_denom.append((tf.square(tf.reduce_sum(y_aux1_train[:,4], axis=-1)/sample_size - \n",
        "                          tf.reduce_sum(y_aux1_train[:,2], axis=-1)/sample_size * \n",
        "                          tf.reduce_sum(y_aux1_train[:,0], axis=-1)/sample_size)))\n",
        "\n",
        "  ## training the model\n",
        "  model_CIDNN, history_CIDNN = train_CIDNN(model = None, X = X_train_tokens, y = y_aux1_train, epochs=300, batch_size = 256)\n",
        "  model_CIDNN.save(\"/content/drive/MyDrive/Can_a_joint_model_assist_target_label_prediction/Toxic_comments/model_CIDNN_\"+str(fold_no))\n",
        "  del model_CIDNN\n",
        "\n",
        "\n",
        "  # Extracting transformed features from the CIDNN\n",
        "  model_CIDNN = keras.models.load_model('/content/drive/MyDrive/Can_a_joint_model_assist_target_label_prediction/Toxic_comments/model_CIDNN_'+str(fold_no), compile=False)\n",
        "  extraction1 = Model(inputs=model_CIDNN.inputs, outputs=model_CIDNN.layers[-2].output)\n",
        "  extracted_features1 = extraction1.predict(X_train_tokens)\n",
        "  del model_CIDNN, extraction1\n",
        "  keras.backend.clear_session()\n",
        "  \n",
        "  # LDR-CIDNN\n",
        "  model_LDR_CIDNN, history_LDR_CIDNN = train_LDR(model = None, X= X_train_tokens, X_extracted = extracted_features1, y = y_tgt_train, epochs= 300, batch_size = 256)\n",
        "  model_LDR_CIDNN.save(\"/content/drive/MyDrive/Can_a_joint_model_assist_target_label_prediction/Toxic_comments/model_LDR_CIDNN_\"+str(fold_no))\n",
        "  del model_LDR_CIDNN\n",
        "  keras.backend.clear_session()\n",
        "\n",
        "  # NLDR-CIDNN\n",
        "  model_NLDR_CIDNN, history_NLDR_CIDNN = train_NLDR(model = None, X= X_train_tokens, X_extracted = extracted_features1, y = y_tgt_train, epochs= 300, batch_size = 256)\n",
        "  model_NLDR_CIDNN.save(\"/content/drive/MyDrive/Can_a_joint_model_assist_target_label_prediction/Toxic_comments/model_NLDR_CIDNN_\"+str(fold_no))\n",
        "  del model_NLDR_CIDNN, extracted_features1\n",
        "  keras.backend.clear_session()\n",
        "\n",
        "  \n",
        "  # Extracting transformed features from the joint model\n",
        "  model_NN_joint = keras.models.load_model('/content/drive/MyDrive/Can_a_joint_model_assist_target_label_prediction/Toxic_comments/model_NN_joint_'+str(fold_no), compile=False)\n",
        "  extraction_ce1 = Model(inputs=model_NN_joint.inputs, outputs=model_NN_joint.layers[-2].output)\n",
        "  extracted_features_ce1 = extraction_ce1.predict(X_train_tokens)\n",
        "  del model_NN_joint, extraction_ce1\n",
        "  keras.backend.clear_session()\n",
        "\n",
        "  # LDR-NN-joint\n",
        "  model_LDR_NN_joint, history_LDR_NN_joint = train_LDR(model = None, X= X_train_tokens, X_extracted = extracted_features_ce1, y = y_tgt_train, epochs= 300, batch_size = 256)\n",
        "  model_LDR_NN_joint.save(\"/content/drive/MyDrive/Can_a_joint_model_assist_target_label_prediction/Toxic_comments/model_LDR_NN_joint_\"+str(fold_no))\n",
        "  del model_LDR_NN_joint\n",
        "  keras.backend.clear_session()\n",
        "\n",
        "  # NLDR-NN-joint\n",
        "  model_NLDR_NN_joint, history_NLDR_NN_joint = train_NLDR(model = None, X = X_train_tokens, X_extracted = extracted_features_ce1, y = y_tgt_train, epochs = 300, batch_size = 256)\n",
        "  model_NLDR_NN_joint.save(\"/content/drive/MyDrive/Can_a_joint_model_assist_target_label_prediction/Toxic_comments/model_NLDR_NN_joint_\"+str(fold_no))\n",
        "  del model_NLDR_NN_joint\n",
        "  keras.backend.clear_session()\n",
        "\n",
        "  del X_train, X_test, y_aux1_train, y_aux1_test, y_tgt_train, y_tgt_test, tokenizer_obj, X_train_tokens, X_test_tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGutK0ZKlnLd"
      },
      "source": [
        "## Alternative code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRGknQeXl_Sl"
      },
      "source": [
        "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
        "from keras.models import Model\n",
        "import keras\n",
        "\n",
        "for fold_no in np.arange(10):\n",
        "  X_train, X_test, y_aux1_train, y_aux1_test = split_data(X, y_aux_agg, fold_no)\n",
        "  X_train, X_test, y_tgt_train, y_tgt_test = split_data(X, y_tgt, fold_no)\n",
        "  y_aux1_train = np.array(y_aux1_train, dtype='float32')\n",
        "\n",
        "  # Create a tf-idf matrix\n",
        "  vocab_size = 5000\n",
        "\n",
        "  tokenizer_obj = Tokenizer(num_words = vocab_size)\n",
        "  tokenizer_obj.fit_on_texts(X_train)\n",
        "\n",
        "  max_length = max([len(s.split()) for s in X])\n",
        "\n",
        "  X_train_tokens = tokenizer_obj.texts_to_matrix(X_train, mode='tfidf')\n",
        "  X_test_tokens = tokenizer_obj.texts_to_matrix(X_test, mode='tfidf')\n",
        "\n",
        "  # Baseline\n",
        "  model_Baseline, history_Baseline = train_Baseline(model = None, X= X_train_tokens, y = y_tgt_train, epochs = 300, batch_size = 256)\n",
        "  model_Baseline.save(\"/content/drive/MyDrive/Can_a_joint_model_assist_target_label_prediction/Toxic_comments/model_Baseline_\"+str(fold_no))\n",
        "  del model_Baseline\n",
        "  keras.backend.clear_session()\n",
        "  \n",
        "  # NN\n",
        "  model_NN, history_NN = train_NN(model = None, X= X_train_tokens, y = y_tgt_train, epochs = 300, batch_size = 256)\n",
        "  model_NN.save(\"/content/drive/MyDrive/Can_a_joint_model_assist_target_label_prediction/Toxic_comments/model_NN_\"+str(fold_no))\n",
        "  del model_NN\n",
        "  keras.backend.clear_session()\n",
        "\n",
        "  # NN-joint\n",
        "  model_NN_joint, history_NN_joint = train_NN(model = None, X= X_train_tokens, y = y_aux1_train[:,0:3], epochs = 300, batch_size = 256)\n",
        "  model_NN_joint.save(\"/content/drive/MyDrive/Can_a_joint_model_assist_target_label_prediction/Toxic_comments/model_NN_joint_\"+str(fold_no))\n",
        "  del model_NN_joint\n",
        "  keras.backend.clear_session()\n",
        "\n",
        "  del X_train, X_test, y_aux1_train, y_aux1_test, y_tgt_train, y_tgt_test, tokenizer_obj, X_train_tokens, X_test_tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1d4R1t2l_yb"
      },
      "source": [
        "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
        "from keras.models import Model\n",
        "import keras\n",
        "\n",
        "for fold_no in np.arange(10):\n",
        "  X_train, X_test, y_aux1_train, y_aux1_test = split_data(X, y_aux_agg, fold_no)\n",
        "  X_train, X_test, y_tgt_train, y_tgt_test = split_data(X, y_tgt, fold_no)\n",
        "  y_aux1_train = np.array(y_aux1_train, dtype='float32')\n",
        "\n",
        "  # Create a tf-idf matrix\n",
        "  vocab_size = 5000\n",
        "\n",
        "  tokenizer_obj = Tokenizer(num_words = vocab_size)\n",
        "  tokenizer_obj.fit_on_texts(X_train)\n",
        "\n",
        "  max_length = max([len(s.split()) for s in X])\n",
        "\n",
        "  X_train_tokens = tokenizer_obj.texts_to_matrix(X_train, mode='tfidf')\n",
        "  X_test_tokens = tokenizer_obj.texts_to_matrix(X_test, mode='tfidf')\n",
        "\n",
        "  # CIDNN\n",
        "\n",
        "  ## the denominator of the first term of CIScore\n",
        "  sample_size = y_aux1_train.shape[0]\n",
        "  CI_denom = []\n",
        "  CI_denom.append((tf.square(tf.reduce_sum(y_aux1_train[:,3], axis=-1)/sample_size - \n",
        "                          tf.reduce_sum(y_aux1_train[:,1], axis=-1)/sample_size * \n",
        "                          tf.reduce_sum(y_aux1_train[:,0], axis=-1)/sample_size)))\n",
        "  CI_denom.append((tf.square(tf.reduce_sum(y_aux1_train[:,4], axis=-1)/sample_size - \n",
        "                          tf.reduce_sum(y_aux1_train[:,2], axis=-1)/sample_size * \n",
        "                          tf.reduce_sum(y_aux1_train[:,0], axis=-1)/sample_size)))\n",
        "\n",
        "  ## training the model\n",
        "  model_CIDNN, history_CIDNN = train_CIDNN(model = None, X = X_train_tokens, y = y_aux1_train, epochs=300, batch_size = 256)\n",
        "  model_CIDNN.save(\"/content/drive/MyDrive/Can_a_joint_model_assist_target_label_prediction/Toxic_comments/model_CIDNN_\"+str(fold_no))\n",
        "  del model_CIDNN\n",
        "\n",
        "\n",
        "  # Extracting transformed features from the CIDNN\n",
        "  model_CIDNN = keras.models.load_model('/content/drive/MyDrive/Can_a_joint_model_assist_target_label_prediction/Toxic_comments/model_CIDNN_'+str(fold_no), compile=False)\n",
        "  extraction1 = Model(inputs=model_CIDNN.inputs, outputs=model_CIDNN.layers[-2].output)\n",
        "  extracted_features1 = extraction1.predict(X_train_tokens)\n",
        "  del model_CIDNN, extraction1\n",
        "  keras.backend.clear_session()\n",
        "  \n",
        "  # LDR-CIDNN\n",
        "  model_LDR_CIDNN, history_LDR_CIDNN = train_LDR(model = None, X= X_train_tokens, X_extracted = extracted_features1, y = y_tgt_train, epochs= 300, batch_size = 256)\n",
        "  model_LDR_CIDNN.save(\"/content/drive/MyDrive/Can_a_joint_model_assist_target_label_prediction/Toxic_comments/model_LDR_CIDNN_\"+str(fold_no))\n",
        "  del model_LDR_CIDNN\n",
        "  keras.backend.clear_session()\n",
        "\n",
        "  # NLDR-CIDNN\n",
        "  model_NLDR_CIDNN, history_NLDR_CIDNN = train_NLDR(model = None, X= X_train_tokens, X_extracted = extracted_features1, y = y_tgt_train, epochs= 300, batch_size = 256)\n",
        "  model_NLDR_CIDNN.save(\"/content/drive/MyDrive/Can_a_joint_model_assist_target_label_prediction/Toxic_comments/model_NLDR_CIDNN_\"+str(fold_no))\n",
        "  del model_NLDR_CIDNN, extracted_features1\n",
        "  keras.backend.clear_session()\n",
        "\n",
        "\n",
        "  del X_train, X_test, y_aux1_train, y_aux1_test, y_tgt_train, y_tgt_test, tokenizer_obj, X_train_tokens, X_test_tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udqLD1E3lmC-"
      },
      "source": [
        "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
        "from keras.models import Model\n",
        "import keras\n",
        "\n",
        "for fold_no in np.arange(10):\n",
        "  X_train, X_test, y_aux1_train, y_aux1_test = split_data(X, y_aux_agg, fold_no)\n",
        "  X_train, X_test, y_tgt_train, y_tgt_test = split_data(X, y_tgt, fold_no)\n",
        "  y_aux1_train = np.array(y_aux1_train, dtype='float32')\n",
        "\n",
        "  # Create a tf-idf matrix\n",
        "  vocab_size = 5000\n",
        "\n",
        "  tokenizer_obj = Tokenizer(num_words = vocab_size)\n",
        "  tokenizer_obj.fit_on_texts(X_train)\n",
        "\n",
        "  max_length = max([len(s.split()) for s in X])\n",
        "\n",
        "  X_train_tokens = tokenizer_obj.texts_to_matrix(X_train, mode='tfidf')\n",
        "  X_test_tokens = tokenizer_obj.texts_to_matrix(X_test, mode='tfidf')\n",
        "  \n",
        "  # Extracting transformed features from the joint model\n",
        "  model_NN_joint = keras.models.load_model('/content/drive/MyDrive/Can_a_joint_model_assist_target_label_prediction/Toxic_comments/model_NN_joint_'+str(fold_no), compile=False)\n",
        "  extraction_ce1 = Model(inputs=model_NN_joint.inputs, outputs=model_NN_joint.layers[-2].output)\n",
        "  extracted_features_ce1 = extraction_ce1.predict(X_train_tokens)\n",
        "  del model_NN_joint, extraction_ce1\n",
        "  keras.backend.clear_session()\n",
        "\n",
        "  # LDR-NN-joint\n",
        "  model_LDR_NN_joint, history_LDR_NN_joint = train_LDR(model = None, X= X_train_tokens, X_extracted = extracted_features_ce1, y = y_tgt_train, epochs= 300, batch_size = 256)\n",
        "  model_LDR_NN_joint.save(\"/content/drive/MyDrive/Can_a_joint_model_assist_target_label_prediction/Toxic_comments/model_LDR_NN_joint_\"+str(fold_no))\n",
        "  del model_LDR_NN_joint\n",
        "  keras.backend.clear_session()\n",
        "\n",
        "  # NLDR-NN-joint\n",
        "  model_NLDR_NN_joint, history_NLDR_NN_joint = train_NLDR(model = None, X = X_train_tokens, X_extracted = extracted_features_ce1, y = y_tgt_train, epochs = 300, batch_size = 256)\n",
        "  model_NLDR_NN_joint.save(\"/content/drive/MyDrive/Can_a_joint_model_assist_target_label_prediction/Toxic_comments/model_NLDR_NN_joint_\"+str(fold_no))\n",
        "  del model_NLDR_NN_joint\n",
        "  keras.backend.clear_session()\n",
        "\n",
        "  del X_train, X_test, y_aux1_train, y_aux1_test, y_tgt_train, y_tgt_test, tokenizer_obj, X_train_tokens, X_test_tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3PhDClwjwQ0"
      },
      "source": [
        "# Evaluation\n",
        "\n",
        "Evaluate models with the following metrics: AUC, accuracy, F1 score. If you do not train models above, please upload the trained models we provide. \n",
        "\n",
        "We provide the trained models (link: https://drive.google.com/drive/folders/1sQaV1LYvmgDbtzhhtBFMavwcJyAI_C-H?usp=sharing). You can download them or copy them to your Google drive. The following link explains how to copy the shared folder to your Google drive: https://stackoverflow.com/questions/54351852/accessing-shared-with-me-with-colab\n",
        "\n",
        "Note that although warning signs might pop up when loading models, it can be ignored because we follow their guideline as they mention\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_LRl2gMT63Z"
      },
      "source": [
        "# if you want to load models from your local drive, please use this code\n",
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFX_Lg-_8uZC"
      },
      "source": [
        "import keras \n",
        "from keras.models import Model\n",
        "from sklearn import metrics\n",
        "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "scores = {}\n",
        "\n",
        "for fold_no in np.arange(10):\n",
        "  X_train, X_test, y_aux1_train, y_aux1_test = split_data(X, y_aux_agg, fold_no)\n",
        "  X_train, X_test, y_tgt_train, y_tgt_test = split_data(X, y_tgt, fold_no)\n",
        "  y_aux1_train = np.array(y_aux1_train, dtype='float32')\n",
        "\n",
        "  vocab_size = 5000\n",
        "\n",
        "  tokenizer_obj = Tokenizer(num_words = vocab_size)\n",
        "  tokenizer_obj.fit_on_texts(X_train)\n",
        "\n",
        "  max_length = max([len(s.split()) for s in X])\n",
        "\n",
        "  X_train_tokens = tokenizer_obj.texts_to_matrix(X_train, mode='tfidf')\n",
        "  X_test_tokens = tokenizer_obj.texts_to_matrix(X_test, mode='tfidf')\n",
        "\n",
        "  model_Baseline = keras.models.load_model('/content/drive/MyDrive/Can_a_joint_model_assist_target_label_prediction/Toxic_comments/model_Baseline_'+str(fold_no))\n",
        "  model_NN = keras.models.load_model('/content/drive/MyDrive/Can_a_joint_model_assist_target_label_prediction/Toxic_comments/model_NN_'+str(fold_no))\n",
        "  model_NN_joint = keras.models.load_model('/content/drive/MyDrive/Can_a_joint_model_assist_target_label_prediction/Toxic_comments/model_NN_joint_'+str(fold_no))\n",
        "  model_CIDNN = keras.models.load_model('/content/drive/MyDrive/Can_a_joint_model_assist_target_label_prediction/Toxic_comments/model_CIDNN_'+str(fold_no), compile=False)\n",
        "  model_NLDR_CIDNN = keras.models.load_model('/content/drive/MyDrive/Can_a_joint_model_assist_target_label_prediction/Toxic_comments/model_NLDR_CIDNN_'+str(fold_no))\n",
        "  model_LDR_CIDNN = keras.models.load_model('/content/drive/MyDrive/Can_a_joint_model_assist_target_label_prediction/Toxic_comments/model_LDR_CIDNN_'+str(fold_no))\n",
        "  model_NLDR_NN_joint = keras.models.load_model('/content/drive/MyDrive/Can_a_joint_model_assist_target_label_prediction/Toxic_comments/model_NLDR_NN_joint_'+str(fold_no))\n",
        "  model_LDR_NN_joint = keras.models.load_model('/content/drive/MyDrive/Can_a_joint_model_assist_target_label_prediction/Toxic_comments/model_LDR_NN_joint_'+str(fold_no))\n",
        "\n",
        "\n",
        "  predicted_Baseline = model_Baseline.predict(X_test_tokens)\n",
        "  predicted_NN = model_NN.predict(X_test_tokens)\n",
        "  predicted_NN_joint = model_NN_joint.predict(X_test_tokens)[:,0]\n",
        "  \n",
        "  #CIDNN\n",
        "  extraction = Model(inputs=model_CIDNN.inputs, outputs=model_CIDNN.layers[-2].output)\n",
        "  predicted_extract = extraction.predict(X_test_tokens)\n",
        "  predicted_LDR_CIDNN = model_LDR_CIDNN.predict(np.hstack((X_test_tokens, predicted_extract)))\n",
        "  predicted_NLDR_CIDNN = model_NLDR_CIDNN.predict(np.hstack((X_test_tokens, predicted_extract)))\n",
        "  \n",
        "  #NN-joint\n",
        "  extraction_NN_joint = Model(inputs=model_NN_joint.inputs, outputs=model_NN_joint.layers[-2].output)\n",
        "  predicted_extract_NN_joint = extraction_NN_joint.predict(X_test_tokens)\n",
        "  predicted_LDR_NN_joint = model_LDR_NN_joint.predict(np.hstack((X_test_tokens, predicted_extract_NN_joint)))\n",
        "  predicted_NLDR_NN_joint = model_NLDR_NN_joint.predict(np.hstack((X_test_tokens, predicted_extract_NN_joint)))\n",
        "  \n",
        "  auc_Baseline = metrics.roc_auc_score(y_tgt_test, (predicted_Baseline).reshape(-1))\n",
        "  auc_NN = metrics.roc_auc_score(y_tgt_test, (predicted_NN).reshape(-1))\n",
        "  auc_NN_joint = metrics.roc_auc_score(y_tgt_test, (predicted_NN_joint).reshape(-1))\n",
        "  auc_LDR_CIDNN = metrics.roc_auc_score(y_tgt_test, (predicted_LDR_CIDNN).reshape(-1))\n",
        "  auc_NLDR_CIDNN = metrics.roc_auc_score(y_tgt_test, (predicted_NLDR_CIDNN).reshape(-1))\n",
        "  auc_NLDR_NN_joint = metrics.roc_auc_score(y_tgt_test, (predicted_NLDR_NN_joint).reshape(-1))\n",
        "  auc_LDR_NN_joint = metrics.roc_auc_score(y_tgt_test, (predicted_LDR_NN_joint).reshape(-1))\n",
        "\n",
        "  acc_Baseline = np.mean(((predicted_Baseline > 0.5) *1).reshape(-1) == y_tgt_test)\n",
        "  acc_NN = np.mean(((predicted_NN > 0.5) *1).reshape(-1) == y_tgt_test)\n",
        "  acc_NN_joint = np.mean(((predicted_NN_joint > 0.5) *1).reshape(-1) == y_tgt_test)\n",
        "  acc_LDR_CIDNN = np.mean(((predicted_LDR_CIDNN > 0.5) *1).reshape(-1) == y_tgt_test)\n",
        "  acc_NLDR_CIDNN = np.mean(((predicted_NLDR_CIDNN > 0.5) *1).reshape(-1) == y_tgt_test)\n",
        "  acc_LDR_NN_joint = np.mean(((predicted_LDR_NN_joint > 0.5) *1).reshape(-1) == y_tgt_test)\n",
        "  acc_NLDR_NN_joint = np.mean(((predicted_NLDR_NN_joint > 0.5) *1).reshape(-1) == y_tgt_test)\n",
        "\n",
        "  f1_Baseline = metrics.f1_score(y_tgt_test, ((predicted_Baseline>0.5) *1).reshape(-1))\n",
        "  f1_NN = metrics.f1_score(y_tgt_test, ((predicted_NN>0.5) *1).reshape(-1))\n",
        "  f1_NN_joint = metrics.f1_score(y_tgt_test, ((predicted_NN_joint>0.5) *1).reshape(-1))\n",
        "  f1_LDR_CIDNN = metrics.f1_score(y_tgt_test, ((predicted_LDR_CIDNN>0.5)*1).reshape(-1))\n",
        "  f1_NLDR_CIDNN = metrics.f1_score(y_tgt_test, ((predicted_NLDR_CIDNN>0.5)*1).reshape(-1))\n",
        "  f1_LDR_NN_joint = metrics.f1_score(y_tgt_test, ((predicted_LDR_NN_joint>0.5)*1).reshape(-1))\n",
        "  f1_NLDR_NN_joint = metrics.f1_score(y_tgt_test, ((predicted_NLDR_NN_joint>0.5)*1).reshape(-1))\n",
        "\n",
        "  scores[fold_no] = np.array([auc_Baseline, auc_NN, auc_NN_joint, auc_LDR_NN_joint, auc_LDR_CIDNN, auc_NLDR_NN_joint, auc_NLDR_CIDNN,  \n",
        "                              acc_Baseline, acc_NN, acc_NN_joint, acc_LDR_NN_joint, acc_LDR_CIDNN, acc_NLDR_NN_joint, acc_NLDR_CIDNN,  \n",
        "                              f1_Baseline, f1_NN, f1_NN_joint, f1_LDR_NN_joint, f1_LDR_CIDNN, f1_NLDR_NN_joint, f1_NLDR_CIDNN])\n",
        "  del X_train, X_test, y_aux1_train, y_aux1_test, y_tgt_train, y_tgt_test, tokenizer_obj, X_train_tokens, X_test_tokens\n",
        "\n",
        "scores_final = np.vstack(scores.values())\n",
        "\n",
        "scores_final = pd.DataFrame(scores_final, columns = ['auc_Baseline','auc_NN', 'auc_NN_joint', 'auc_LDR_NN_joint', 'auc_LDR_CIDNN', 'auc_NLDR_NN_joint', 'auc_NLDR_CIDNN',\n",
        "                                                     'acc_Baseline','acc_NN', 'acc_NN_joint', 'acc_LDR_NN_joint', 'acc_LDR_CIDNN','acc_NLDR_NN_joint', 'acc_NLDR_CIDNN',\n",
        "                                                     'f1_Baseline','f1_NN', 'f1_NN_joint', 'f1_LDR_NN_joint','f1_LDR_CIDNN','f1_NLDR_NN_joint', 'f1_NLDR_CIDNN'])\n",
        "# scores_final.to_csv('/content/drive/MyDrive/Can_a_joint_model_assist_target_label_prediction/Toxic_comments/scores_final.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        },
        "id": "WIt9qpG-cZpH",
        "outputId": "29a6a140-3494-4c23-bb92-c2b135337788"
      },
      "source": [
        "scores_final"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>auc_Baseline</th>\n",
              "      <th>auc_NN</th>\n",
              "      <th>auc_NN_joint</th>\n",
              "      <th>auc_LDR_NN_joint</th>\n",
              "      <th>auc_LDR_CIDNN</th>\n",
              "      <th>auc_NLDR_NN_joint</th>\n",
              "      <th>auc_NLDR_CIDNN</th>\n",
              "      <th>acc_Baseline</th>\n",
              "      <th>acc_NN</th>\n",
              "      <th>acc_NN_joint</th>\n",
              "      <th>acc_LDR_NN_joint</th>\n",
              "      <th>acc_LDR_CIDNN</th>\n",
              "      <th>acc_NLDR_NN_joint</th>\n",
              "      <th>acc_NLDR_CIDNN</th>\n",
              "      <th>f1_Baseline</th>\n",
              "      <th>f1_NN</th>\n",
              "      <th>f1_NN_joint</th>\n",
              "      <th>f1_LDR_NN_joint</th>\n",
              "      <th>f1_LDR_CIDNN</th>\n",
              "      <th>f1_NLDR_NN_joint</th>\n",
              "      <th>f1_NLDR_CIDNN</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.957367</td>\n",
              "      <td>0.957201</td>\n",
              "      <td>0.958697</td>\n",
              "      <td>0.958348</td>\n",
              "      <td>0.958291</td>\n",
              "      <td>0.958895</td>\n",
              "      <td>0.957172</td>\n",
              "      <td>0.956072</td>\n",
              "      <td>0.958328</td>\n",
              "      <td>0.958391</td>\n",
              "      <td>0.958704</td>\n",
              "      <td>0.958328</td>\n",
              "      <td>0.958829</td>\n",
              "      <td>0.958892</td>\n",
              "      <td>0.727555</td>\n",
              "      <td>0.753795</td>\n",
              "      <td>0.753160</td>\n",
              "      <td>0.755473</td>\n",
              "      <td>0.757211</td>\n",
              "      <td>0.763414</td>\n",
              "      <td>0.765882</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.959892</td>\n",
              "      <td>0.961078</td>\n",
              "      <td>0.962976</td>\n",
              "      <td>0.962125</td>\n",
              "      <td>0.963878</td>\n",
              "      <td>0.961999</td>\n",
              "      <td>0.963442</td>\n",
              "      <td>0.955881</td>\n",
              "      <td>0.958451</td>\n",
              "      <td>0.958764</td>\n",
              "      <td>0.958263</td>\n",
              "      <td>0.959454</td>\n",
              "      <td>0.957385</td>\n",
              "      <td>0.958514</td>\n",
              "      <td>0.738679</td>\n",
              "      <td>0.766796</td>\n",
              "      <td>0.769446</td>\n",
              "      <td>0.765658</td>\n",
              "      <td>0.773855</td>\n",
              "      <td>0.768707</td>\n",
              "      <td>0.775136</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.953628</td>\n",
              "      <td>0.954067</td>\n",
              "      <td>0.957461</td>\n",
              "      <td>0.957377</td>\n",
              "      <td>0.955862</td>\n",
              "      <td>0.956374</td>\n",
              "      <td>0.953846</td>\n",
              "      <td>0.956320</td>\n",
              "      <td>0.958388</td>\n",
              "      <td>0.959266</td>\n",
              "      <td>0.958952</td>\n",
              "      <td>0.958200</td>\n",
              "      <td>0.958200</td>\n",
              "      <td>0.957385</td>\n",
              "      <td>0.728477</td>\n",
              "      <td>0.757487</td>\n",
              "      <td>0.760324</td>\n",
              "      <td>0.758570</td>\n",
              "      <td>0.756480</td>\n",
              "      <td>0.760503</td>\n",
              "      <td>0.757835</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.956152</td>\n",
              "      <td>0.955841</td>\n",
              "      <td>0.959795</td>\n",
              "      <td>0.959054</td>\n",
              "      <td>0.956158</td>\n",
              "      <td>0.958439</td>\n",
              "      <td>0.954265</td>\n",
              "      <td>0.955192</td>\n",
              "      <td>0.957573</td>\n",
              "      <td>0.957511</td>\n",
              "      <td>0.957385</td>\n",
              "      <td>0.957323</td>\n",
              "      <td>0.956571</td>\n",
              "      <td>0.956445</td>\n",
              "      <td>0.723190</td>\n",
              "      <td>0.751924</td>\n",
              "      <td>0.748703</td>\n",
              "      <td>0.747961</td>\n",
              "      <td>0.751369</td>\n",
              "      <td>0.749004</td>\n",
              "      <td>0.752757</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.956634</td>\n",
              "      <td>0.955512</td>\n",
              "      <td>0.959914</td>\n",
              "      <td>0.958964</td>\n",
              "      <td>0.957868</td>\n",
              "      <td>0.957716</td>\n",
              "      <td>0.954681</td>\n",
              "      <td>0.956195</td>\n",
              "      <td>0.958263</td>\n",
              "      <td>0.958952</td>\n",
              "      <td>0.957949</td>\n",
              "      <td>0.957636</td>\n",
              "      <td>0.958451</td>\n",
              "      <td>0.957761</td>\n",
              "      <td>0.730220</td>\n",
              "      <td>0.756044</td>\n",
              "      <td>0.759809</td>\n",
              "      <td>0.754662</td>\n",
              "      <td>0.754539</td>\n",
              "      <td>0.764309</td>\n",
              "      <td>0.762006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.960960</td>\n",
              "      <td>0.959389</td>\n",
              "      <td>0.963054</td>\n",
              "      <td>0.963059</td>\n",
              "      <td>0.961313</td>\n",
              "      <td>0.962841</td>\n",
              "      <td>0.960132</td>\n",
              "      <td>0.953751</td>\n",
              "      <td>0.956195</td>\n",
              "      <td>0.957887</td>\n",
              "      <td>0.958012</td>\n",
              "      <td>0.957135</td>\n",
              "      <td>0.957135</td>\n",
              "      <td>0.956132</td>\n",
              "      <td>0.723181</td>\n",
              "      <td>0.750089</td>\n",
              "      <td>0.761364</td>\n",
              "      <td>0.762580</td>\n",
              "      <td>0.754839</td>\n",
              "      <td>0.763158</td>\n",
              "      <td>0.757618</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.950740</td>\n",
              "      <td>0.950716</td>\n",
              "      <td>0.955201</td>\n",
              "      <td>0.954412</td>\n",
              "      <td>0.950463</td>\n",
              "      <td>0.953280</td>\n",
              "      <td>0.947166</td>\n",
              "      <td>0.954816</td>\n",
              "      <td>0.957761</td>\n",
              "      <td>0.957887</td>\n",
              "      <td>0.957448</td>\n",
              "      <td>0.956759</td>\n",
              "      <td>0.957949</td>\n",
              "      <td>0.957761</td>\n",
              "      <td>0.729049</td>\n",
              "      <td>0.761837</td>\n",
              "      <td>0.761364</td>\n",
              "      <td>0.759134</td>\n",
              "      <td>0.753571</td>\n",
              "      <td>0.769178</td>\n",
              "      <td>0.764500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.954937</td>\n",
              "      <td>0.953600</td>\n",
              "      <td>0.958282</td>\n",
              "      <td>0.957060</td>\n",
              "      <td>0.955234</td>\n",
              "      <td>0.957583</td>\n",
              "      <td>0.952810</td>\n",
              "      <td>0.956132</td>\n",
              "      <td>0.958451</td>\n",
              "      <td>0.958514</td>\n",
              "      <td>0.959078</td>\n",
              "      <td>0.957824</td>\n",
              "      <td>0.959078</td>\n",
              "      <td>0.957511</td>\n",
              "      <td>0.728050</td>\n",
              "      <td>0.758646</td>\n",
              "      <td>0.763909</td>\n",
              "      <td>0.765530</td>\n",
              "      <td>0.755184</td>\n",
              "      <td>0.772077</td>\n",
              "      <td>0.761603</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.958331</td>\n",
              "      <td>0.957567</td>\n",
              "      <td>0.962303</td>\n",
              "      <td>0.962487</td>\n",
              "      <td>0.958876</td>\n",
              "      <td>0.960863</td>\n",
              "      <td>0.957279</td>\n",
              "      <td>0.956821</td>\n",
              "      <td>0.958827</td>\n",
              "      <td>0.959391</td>\n",
              "      <td>0.959266</td>\n",
              "      <td>0.958827</td>\n",
              "      <td>0.959579</td>\n",
              "      <td>0.957949</td>\n",
              "      <td>0.738520</td>\n",
              "      <td>0.762387</td>\n",
              "      <td>0.765387</td>\n",
              "      <td>0.764322</td>\n",
              "      <td>0.763243</td>\n",
              "      <td>0.772487</td>\n",
              "      <td>0.764479</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.954149</td>\n",
              "      <td>0.953655</td>\n",
              "      <td>0.959154</td>\n",
              "      <td>0.957620</td>\n",
              "      <td>0.953724</td>\n",
              "      <td>0.956522</td>\n",
              "      <td>0.951935</td>\n",
              "      <td>0.957636</td>\n",
              "      <td>0.959078</td>\n",
              "      <td>0.959579</td>\n",
              "      <td>0.959328</td>\n",
              "      <td>0.958263</td>\n",
              "      <td>0.958890</td>\n",
              "      <td>0.958952</td>\n",
              "      <td>0.747573</td>\n",
              "      <td>0.770151</td>\n",
              "      <td>0.769561</td>\n",
              "      <td>0.768131</td>\n",
              "      <td>0.765328</td>\n",
              "      <td>0.773637</td>\n",
              "      <td>0.773748</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   auc_Baseline    auc_NN  ...  f1_NLDR_NN_joint  f1_NLDR_CIDNN\n",
              "0      0.957367  0.957201  ...          0.763414       0.765882\n",
              "1      0.959892  0.961078  ...          0.768707       0.775136\n",
              "2      0.953628  0.954067  ...          0.760503       0.757835\n",
              "3      0.956152  0.955841  ...          0.749004       0.752757\n",
              "4      0.956634  0.955512  ...          0.764309       0.762006\n",
              "5      0.960960  0.959389  ...          0.763158       0.757618\n",
              "6      0.950740  0.950716  ...          0.769178       0.764500\n",
              "7      0.954937  0.953600  ...          0.772077       0.761603\n",
              "8      0.958331  0.957567  ...          0.772487       0.764479\n",
              "9      0.954149  0.953655  ...          0.773637       0.773748\n",
              "\n",
              "[10 rows x 21 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZ-BQACIccqw",
        "outputId": "92aefd8f-b0ad-4f9c-d703-a58d100b7175"
      },
      "source": [
        "np.mean(scores_final)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "auc_Baseline         0.956279\n",
              "auc_NN               0.955862\n",
              "auc_NN_joint         0.959684\n",
              "auc_LDR_NN_joint     0.959050\n",
              "auc_LDR_CIDNN        0.957167\n",
              "auc_NLDR_NN_joint    0.958451\n",
              "auc_NLDR_CIDNN       0.955273\n",
              "acc_Baseline         0.955882\n",
              "acc_NN               0.958131\n",
              "acc_NN_joint         0.958614\n",
              "acc_LDR_NN_joint     0.958439\n",
              "acc_LDR_CIDNN        0.957975\n",
              "acc_NLDR_NN_joint    0.958207\n",
              "acc_NLDR_CIDNN       0.957730\n",
              "f1_Baseline          0.731449\n",
              "f1_NN                0.758916\n",
              "f1_NN_joint          0.761303\n",
              "f1_LDR_NN_joint      0.760202\n",
              "f1_LDR_CIDNN         0.758562\n",
              "f1_NLDR_NN_joint     0.765647\n",
              "f1_NLDR_CIDNN        0.763556\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    }
  ]
}