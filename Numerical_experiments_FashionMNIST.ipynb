{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Numerical experiments - FashionMNIST.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyO7M8/Sf3n21ITcreYLA2rp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaeyoung-jae-park/Joint-model-assisted-Decision-Rule/blob/main/Numerical_experiments_FashionMNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NyhN2XuaLq5"
      },
      "source": [
        "# Data preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8K8R3kZhafkE"
      },
      "source": [
        "## Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiawKlmNMQY3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11e66937-f08a-46a9-8b28-846611741b20"
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "(X, y), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "X = X/255\n",
        "X = np.array(X).reshape((60000, 28,28,1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCb2FAvLI1GL",
        "outputId": "9bfe4fb6-fe9b-4126-f1d7-07f03af10d4f"
      },
      "source": [
        "pd.DataFrame(np.repeat(np.arange(10), 6000)).value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9    6000\n",
              "8    6000\n",
              "7    6000\n",
              "6    6000\n",
              "5    6000\n",
              "4    6000\n",
              "3    6000\n",
              "2    6000\n",
              "1    6000\n",
              "0    6000\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R62mFvD0QOSg",
        "outputId": "f80fea84-c39d-43e4-cde2-541889867ea8"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJhKMp7TbDvZ"
      },
      "source": [
        "## Label setting\n",
        "Target - Shirt\n",
        "\n",
        "Auxiliary outcomes - t-shirt/top/shirt and coat/shirt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhR1ccZ_mRl4"
      },
      "source": [
        "y_aux = np.vstack(((y == 0)*1 + (y == 6)*1, (y == 4)*1 + (y==6)*1))\n",
        "y_tgt = (y == 6) * 1\n",
        "y_aux_agg = np.transpose(np.vstack((y_tgt, y_aux, y_tgt*y_aux[0,:], y_tgt*y_aux[1,:])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsQoHzdnr41q",
        "outputId": "2c48859a-b2c3-482c-dfbf-b191197377d8"
      },
      "source": [
        "np.sum(y_aux_agg, axis=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 6000, 12000, 12000,  6000,  6000])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qZa0N47bY2m"
      },
      "source": [
        "# Hyperparameter settings\n",
        "\n",
        "Neural network structures are defined for the following seven models: Baseline, NN, NN-joint, LDR-NN-joint, LDR-CIDNN, NLDR-NN-joint, and NLDR-CIDNN.\n",
        "\n",
        "The optimizer is stochastic gradient descent with a learning rate of $10^{-3}$ and a momentum of 0.9. \n",
        "For each training set (9 folds), we further split it and use 20\\% of the data as the validation set to avoid overfitting. \n",
        "The learning rate will decrease and the training may stop early based on the validation loss values. \n",
        "When a validation loss value does not achieve one smaller than the minimum of the last 5 epochs, the learning rate reduces to one tenth of its previous value. Further, although the total number of epochs is 300, the training will stop early, if a loss value smaller than the minimum is not obtained within 10 epochs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-QOYcGZHMIy"
      },
      "source": [
        "## Baseline\n",
        "\n",
        "A linear decision rule does not have hidden layers. The network only consists of an input layer and an output layer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYh3v4bAdz2P"
      },
      "source": [
        "from keras.models import Sequential, Model, Input\n",
        "from keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten, concatenate\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from keras.optimizers import SGD\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cT80YgHeynd"
      },
      "source": [
        "def define_Baseline(output_dim):\n",
        "  output_dim = output_dim\n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(Flatten(input_shape = (28, 28, 1)))\n",
        "  model.add(Dense(output_dim, activation = \"sigmoid\"))\n",
        "  model.summary()\n",
        "\n",
        "  return model\n",
        "\n",
        "def train_Baseline(model, X, y, epochs, batch_size):\n",
        "  if model is None:\n",
        "    if len(y.shape) == 1:\n",
        "      model = define_Baseline(1)\n",
        "    else: model= define_Baseline(y.shape[1])\n",
        "  es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
        "  rl = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5)\n",
        "\n",
        "  model.compile(loss='binary_crossentropy', optimizer=SGD(learning_rate = 0.001, momentum=0.9), metrics=['accuracy'])\n",
        "  history = model.fit(X , y, batch_size = batch_size, epochs = epochs, validation_split=0.2, callbacks=[es, rl], verbose=2)\n",
        "  return model, history\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIs_5aZKHTYA"
      },
      "source": [
        "## NN and NN-joint\n",
        "\n",
        "The network stacks two pairs of convolution and max pooling, a ﬂatten layer, a dropout layer with a rate of 0.5, and a fully-connected hidden layer containing 512 units. The ﬁrst pair includes a convolution layer with 64 ﬁlters and a kernel size of 7-by-7, and a max pooling layer with a size of 2-by-2. The second pair includes a convolution layer with 128 ﬁlters and a kernel size of 5-by-5, and a max pooling layer with a size of 2-by-2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCk6dkJWPfCm"
      },
      "source": [
        "def define_NN(output_dim):\n",
        "  output_dim = output_dim\n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(64, 7, input_shape = (28, 28, 1), padding='same', activation = \"relu\"))\n",
        "  model.add(MaxPooling2D(pool_size = (2,2)))\n",
        "  model.add(Conv2D(128, 5, activation = \"relu\", padding='same'))\n",
        "  model.add(MaxPooling2D(pool_size = (2,2)))\n",
        "  \n",
        "  model.add(Flatten())\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(512, activation=\"relu\"))\n",
        "  model.add(Dense(output_dim, activation = \"sigmoid\"))\n",
        "  model.summary()\n",
        "\n",
        "  return model\n",
        "\n",
        "def train_NN(model, X, y, epochs, batch_size):\n",
        "  if model is None:\n",
        "    if len(y.shape) == 1:\n",
        "      model = define_NN(1)\n",
        "    else: model= define_NN(y.shape[1])\n",
        "  es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
        "  rl = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5)\n",
        "\n",
        "\n",
        "  model.compile(loss='binary_crossentropy', optimizer=SGD(learning_rate = 0.001, momentum=0.9), metrics=['accuracy'])\n",
        "  history = model.fit(X , y, batch_size = batch_size, epochs = epochs, validation_split=0.2, callbacks=[es, rl], verbose=2)\n",
        "  \n",
        "  return model, history\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ai9qtwJofi8M"
      },
      "source": [
        "## CIDNN\n",
        "\n",
        "To calculate CIScore correctly, the indicies should match the the positions of auxiliary outcomes. The output layer and loss function for CIDNN is different from those for NN, whereas the input layer and hidden layers of CIDNN are identical."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xz3GkSOm2d9e"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def CIScore(y_true, y_pred):\n",
        "  CI_numer = []\n",
        "  CI_numer.append(tf.square((y_pred[:, 3] - y_pred[:, 1] * y_pred[:, 0])))\n",
        "  CI_numer.append(tf.square((y_pred[:, 4] - y_pred[:, 2] * y_pred[:, 0])))\n",
        "\n",
        "  return tf.reduce_mean(CI_numer[0], axis=-1)/CI_denom[0] + tf.reduce_mean(CI_numer[1], axis=-1)/CI_denom[1] + tf.keras.losses.binary_crossentropy(y_true[:,0], y_pred[:,0])\n",
        "\n",
        "def train_CIDNN(model, X, y, epochs, batch_size):\n",
        "  if model is None:\n",
        "    model = define_NN(y.shape[1])\n",
        "\n",
        "  es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
        "  rl = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5)\n",
        "\n",
        "  model.compile(loss=CIScore, optimizer=SGD(learning_rate = 0.001, momentum=0.9), metrics=['accuracy'])\n",
        "  history = model.fit(X , y, batch_size = batch_size, epochs = epochs, validation_split=0.2, callbacks=[es, rl], verbose=2)\n",
        "  return model, history\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84q3LirUp796"
      },
      "source": [
        "## Joint-model-assisted linear/nonlinear decision rules (LDR/NLDR)\n",
        "\n",
        "For LDR, the original X is flattened first, and then the flattened X and transformed X are concatenated for the input. For NLDR, the concatenation is performed once convolutions are done."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWBwLpGYfjNm"
      },
      "source": [
        "def define_LDR(input_shape2):\n",
        "\n",
        "  inputA = Input(shape = (28,28,1,))\n",
        "  inputB = Input(shape = (input_shape2,))\n",
        "\n",
        "  modelA = Flatten()(inputA)\n",
        "  modelA = Model(inputs = inputA, outputs = modelA)\n",
        "\n",
        "  combined = concatenate([modelA.output, inputB])\n",
        "  modelB = Dense(1, activation = 'sigmoid')(combined)\n",
        "\n",
        "  model_transf = Model(inputs = [modelA.input, inputB], outputs = modelB)\n",
        "  model_transf.summary()\n",
        "  return model_transf\n",
        "\n",
        "def train_LDR(model, X, X_extracted, y, epochs, batch_size):\n",
        "  \n",
        "  if model is None:\n",
        "    model = define_LDR(X_extracted.shape[1])\n",
        "\n",
        "  es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
        "  rl = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5)\n",
        "\n",
        "  model.compile(loss='binary_crossentropy', optimizer=SGD(learning_rate = 0.001, momentum=0.9), metrics=['accuracy'])\n",
        "  history = model.fit([X,X_extracted] , y, batch_size = batch_size, epochs = epochs, validation_split=0.2, callbacks=[es, rl], verbose=2)\n",
        "  return model, history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFtqbNIKDmR4"
      },
      "source": [
        "def define_NLDR(input_shape2):\n",
        "\n",
        "  inputA = Input(shape = (28,28,1,))\n",
        "  inputB = Input(shape = (input_shape2,))\n",
        "\n",
        "  modelA = Conv2D(64, 7, input_shape = (28, 28, 1), padding='same', activation = \"relu\")(inputA)\n",
        "  modelA = MaxPooling2D(pool_size = (2,2))(modelA)\n",
        "  modelA = Conv2D(128, 5, activation = \"relu\", padding='same')(modelA)\n",
        "  modelA = MaxPooling2D(pool_size = (2,2))(modelA)\n",
        "  modelA = Flatten()(modelA)\n",
        "  modelA = (Dropout(0.5))(modelA)\n",
        "  modelA = Model(inputs = inputA, outputs = modelA)\n",
        "\n",
        "  combined = concatenate([modelA.output, inputB])\n",
        "  modelB = Dense(512, activation = 'relu')(combined)\n",
        "  modelB = Dense(1, activation = 'sigmoid')(modelB)\n",
        "\n",
        "  model_transf = Model(inputs = [modelA.input, inputB], outputs = modelB)\n",
        "  model_transf.summary()\n",
        "  return model_transf\n",
        "\n",
        "def train_NLDR(model, X, X_extracted, y, epochs, batch_size):\n",
        "  \n",
        "  if model is None:\n",
        "    model = define_NLDR(X_extracted.shape[1])\n",
        "\n",
        "  es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
        "  rl = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5)\n",
        "  model.compile(loss='binary_crossentropy', optimizer=SGD(learning_rate = 0.001, momentum=0.9), metrics=['accuracy'])\n",
        "  history = model.fit([X,X_extracted] , y, batch_size = batch_size, epochs = epochs, validation_split=0.2, callbacks=[es, rl], verbose=2)\n",
        "  return model, history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqJ_IQbEz80U"
      },
      "source": [
        "# Training\n",
        "\n",
        "We use 10-fold cross-validation for the overall procedure. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhjVNq160-OG"
      },
      "source": [
        "y_tgt = np.array(y_tgt)\n",
        "\n",
        "np.random.seed(42)\n",
        "fold_idx = np.random.choice(np.hstack((np.repeat(np.arange(10), int(X.shape[0]/10)), np.arange(X.shape[0] % 10))), size=X.shape[0], replace=False)\n",
        "\n",
        "def split_data(X, y, fold_no):\n",
        "  X_train, X_test, y_train, y_test = X[fold_idx != fold_no], X[fold_idx == fold_no], y[fold_idx != fold_no], y[fold_idx == fold_no]\n",
        "  np.random.seed(fold_no)\n",
        "  shuffle_train = np.random.choice(X_train.shape[0], X_train.shape[0], replace=False)\n",
        "  shuffle_test = np.random.choice(X_test.shape[0], X_test.shape[0], replace=False)\n",
        "  return X_train[shuffle_train], X_test[shuffle_test], y_train[shuffle_train], y_test[shuffle_test]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cw1MVNk1jfdk"
      },
      "source": [
        "We strongly encourage you to mount Google drive with the given directory, '/content/drive' to reproduce our work easily. Also, it takes long time to save and load models from your local drive. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5ucDJdLXvp9",
        "outputId": "162362a0-c2ba-4d6f-9712-815f0e49e76b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IimzI1J0z8Bl"
      },
      "source": [
        "import keras\n",
        "for fold_no in np.arange(10):\n",
        "\n",
        "  X_train, X_test, y_aux_train, y_aux_test = split_data(X, y_aux_agg, fold_no)\n",
        "  X_train, X_test, y_tgt_train, y_tgt_test = split_data(X, y_tgt, fold_no)\n",
        "  y_aux_train = np.array(y_aux_train, dtype='float32')\n",
        "  sample_size = y_aux_train.shape[0]\n",
        "  \n",
        "\n",
        "  # Baseline\n",
        "  model_Baseline, history_Baseline = train_Baseline(model=None, X=X_train, y=y_tgt_train, epochs = 300, batch_size = 256)\n",
        "  model_Baseline.save(\"/content/drive/MyDrive/Can_a_joint_model_assist_target_label_prediction/FashionMNIST/model_Baseline\"+str(fold_no))\n",
        "  keras.backend.clear_session()\n",
        "\n",
        "  # NN\n",
        "  model_NN, history_NN = train_NN(model=None, X=X_train, y=y_tgt_train, epochs = 300, batch_size = 256)\n",
        "  model_NN.save(\"/content/drive/MyDrive/Can_a_joint_model_assist_target_label_prediction/FashionMNIST/model_NN\"+str(fold_no))\n",
        "  keras.backend.clear_session()\n",
        "\n",
        "  # NN joint\n",
        "  model_NN_joint, history_NN_joint =  train_NN(model =None, X=X_train, y=y_aux_train[:,0:3], epochs = 300, batch_size = 256)\n",
        "  model_NN_joint.save(\"/content/drive/MyDrive/Can_a_joint_model_assist_target_label_prediction/FashionMNIST/model_NN_joint\"+str(fold_no))\n",
        "  keras.backend.clear_session()\n",
        "\n",
        "  # CIDNN\n",
        "  ## the denominator of the first term of CIScore\n",
        "  CI_denom = []\n",
        "  CI_denom.append((tf.square(tf.reduce_sum(y_aux_train[:,3], axis=-1)/sample_size - \n",
        "                          tf.reduce_sum(y_aux_train[:,1], axis=-1)/sample_size * \n",
        "                          tf.reduce_sum(y_aux_train[:,0], axis=-1)/sample_size)))\n",
        "  CI_denom.append((tf.square(tf.reduce_sum(y_aux_train[:,4], axis=-1)/sample_size - \n",
        "                          tf.reduce_sum(y_aux_train[:,2], axis=-1)/sample_size * \n",
        "                          tf.reduce_sum(y_aux_train[:,0], axis=-1)/sample_size)))\n",
        "  ## training the model  \n",
        "  model_CIDNN, history_CIDNN = train_CIDNN(model=None, X=X_train, y=y_aux_train, epochs=300, batch_size = 256)\n",
        "  model_CIDNN.save(\"/content/drive/MyDrive/Can_a_joint_model_assist_target_label_prediction/FashionMNIST/model_CIDNN\"+str(fold_no))\n",
        "  keras.backend.clear_session()\n",
        "\n",
        "  # Extracting transformed features from the CIDNN\n",
        "  model_CIDNN = keras.models.load_model(\"/content/drive/MyDrive/Can_a_joint_model_assist_target_label_prediction/FashionMNIST/model_CIDNN\"+str(fold_no), compile= False)\n",
        "  extraction = Model(inputs=model_CIDNN.inputs, outputs=model_CIDNN.layers[-2].output)\n",
        "  extracted_features = extraction.predict(X_train)\n",
        "\n",
        "  # LDR-CIDNN\n",
        "  model_LDR_CIDNN, history_LDR_CIDNN = train_LDR(model=None, X=X_train, X_extracted= extracted_features, y=y_tgt_train, epochs=300, batch_size=256)\n",
        "  model_LDR_CIDNN.save(\"/content/drive/MyDrive/Can_a_joint_model_assist_target_label_prediction/FashionMNIST/model_LDR_CIDNN\"+str(fold_no))\n",
        "  keras.backend.clear_session()\n",
        "\n",
        "  # NLDR-CIDNN\n",
        "  model_NLDR_CIDNN, history_NLDR_CIDNN = train_NLDR(model=None, X=X_train, X_extracted= extracted_features, y=y_tgt_train, epochs=300, batch_size=256)\n",
        "  model_NLDR_CIDNN.save(\"/content/drive/MyDrive/Can_a_joint_model_assist_target_label_prediction/FashionMNIST/model_NLDR_CIDNN\"+str(fold_no))\n",
        "  keras.backend.clear_session()\n",
        "\n",
        "  # Extracting transformed features from the joint model\n",
        "  model_NN_joint = keras.models.load_model(\"/content/drive/MyDrive/Can_a_joint_model_assist_target_label_prediction/FashionMNIST/model_NN_joint\"+str(fold_no), compile=False)\n",
        "  extraction_ce = Model(inputs=model_NN_joint.inputs, outputs=model_NN_joint.layers[-2].output)\n",
        "  extracted_features_ce = extraction_ce.predict(X_train)\n",
        "  \n",
        "  # LDR-NN-joint\n",
        "  model_LDR_NN_joint, history_LDR_NN_joint = train_LDR(model=None, X=X_train, X_extracted= extracted_features_ce, y=y_tgt_train, epochs=300, batch_size=256)\n",
        "  model_LDR_NN_joint.save(\"/content/drive/MyDrive/Can_a_joint_model_assist_target_label_prediction/FashionMNIST/model_LDR_NN_joint\"+str(fold_no))\n",
        "  keras.backend.clear_session()\n",
        "\n",
        "  # NLDR-NN-joint\n",
        "  model_NLDR_NN_joint, history_NLDR_NN_joint = train_NLDR(model=None, X=X_train, X_extracted= extracted_features_ce, y=y_tgt_train, epochs=300, batch_size=256)\n",
        "  model_NLDR_NN_joint.save(\"/content/drive/MyDrive/Can_a_joint_model_assist_target_label_prediction/FashionMNIST/model_NLDR_NN_joint\"+str(fold_no))\n",
        "  keras.backend.clear_session()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCZ3JcWF5gXF"
      },
      "source": [
        "# Evaluation\n",
        "\n",
        "Evaluate models with the following metrics: AUC, accuracy, F1 score. If you do not train models above, please upload the trained models we provide. \n",
        "\n",
        "We provide the trained models (link: https://drive.google.com/drive/folders/1sQaV1LYvmgDbtzhhtBFMavwcJyAI_C-H?usp=sharing). You can download them or copy them to your Google drive. The following link explains how to copy the shared folder to your Google drive: https://stackoverflow.com/questions/54351852/accessing-shared-with-me-with-colab\n",
        "\n",
        "Note that although warning signs might pop up when loading models, it can be ignored because we follow their guideline as they mention\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Ja6d3coeFPt"
      },
      "source": [
        "# if you want to load models from your local drive, please use this code\n",
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwNsaj4GIFIO"
      },
      "source": [
        "import keras \n",
        "from keras.models import Model\n",
        "from sklearn import metrics\n",
        "\n",
        "scores = {}\n",
        "\n",
        "for fold_no in np.arange(10):\n",
        "  X_train, X_test, y_aux_train, y_aux_test = split_data(X, y_aux_agg, fold_no)\n",
        "  X_train, X_test, y_tgt_train, y_tgt_test = split_data(X, y_tgt, fold_no)\n",
        "  y_aux_train = np.array(y_aux_train, dtype='float32')\n",
        "\n",
        "  model_Baseline = keras.models.load_model('/content/drive/MyDrive/Can_a_joint_model_assist_target_label_prediction/FashionMNIST/model_Baseline'+str(fold_no))\n",
        "  model_NN = keras.models.load_model('/content/drive/MyDrive/Can_a_joint_model_assist_target_label_prediction/FashionMNIST/model_NN'+str(fold_no))\n",
        "  model_NN_joint = keras.models.load_model('/content/drive/MyDrive/Can_a_joint_model_assist_target_label_prediction/FashionMNIST/model_NN_joint'+str(fold_no))\n",
        "  model_CIDNN = keras.models.load_model('/content/drive/MyDrive/Can_a_joint_model_assist_target_label_prediction/FashionMNIST/model_CIDNN'+str(fold_no), compile=False)\n",
        "  model_NLDR_CIDNN = keras.models.load_model('/content/drive/MyDrive/Can_a_joint_model_assist_target_label_prediction/FashionMNIST/model_NLDR_CIDNN'+str(fold_no))\n",
        "  model_LDR_CIDNN = keras.models.load_model('/content/drive/MyDrive/Can_a_joint_model_assist_target_label_prediction/FashionMNIST/model_LDR_CIDNN'+str(fold_no))\n",
        "  model_NLDR_NN_joint = keras.models.load_model('/content/drive/MyDrive/Can_a_joint_model_assist_target_label_prediction/FashionMNIST/model_NLDR_NN_joint'+str(fold_no))\n",
        "  model_LDR_NN_joint = keras.models.load_model('/content/drive/MyDrive/Can_a_joint_model_assist_target_label_prediction/FashionMNIST/model_LDR_NN_joint'+str(fold_no))\n",
        "\n",
        "\n",
        "  predicted_Baseline = model_Baseline.predict(X_test)\n",
        "  predicted_NN = model_NN.predict(X_test)\n",
        "  predicted_NN_joint = model_NN_joint.predict(X_test)[:,0]\n",
        "  \n",
        "  #CIDNN\n",
        "  extraction = Model(inputs=model_CIDNN.inputs, outputs=model_CIDNN.layers[-2].output)\n",
        "  predicted_extract = extraction.predict(X_test)\n",
        "  predicted_LDR_CIDNN = model_LDR_CIDNN.predict([X_test, predicted_extract])\n",
        "  predicted_NLDR_CIDNN = model_NLDR_CIDNN.predict([X_test, predicted_extract])\n",
        "  \n",
        "  #NN-joint\n",
        "  extraction_NN_joint = Model(inputs=model_NN_joint.inputs, outputs=model_NN_joint.layers[-2].output)\n",
        "  predicted_extract_NN_joint = extraction_NN_joint.predict(X_test)\n",
        "  predicted_LDR_NN_joint = model_LDR_NN_joint.predict([X_test, predicted_extract_NN_joint])\n",
        "  predicted_NLDR_NN_joint = model_NLDR_NN_joint.predict([X_test, predicted_extract_NN_joint])\n",
        "  \n",
        "  auc_Baseline = metrics.roc_auc_score(y_tgt_test, (predicted_Baseline).reshape(-1))\n",
        "  auc_NN = metrics.roc_auc_score(y_tgt_test, (predicted_NN).reshape(-1))\n",
        "  auc_NN_joint = metrics.roc_auc_score(y_tgt_test, (predicted_NN_joint).reshape(-1))\n",
        "  auc_LDR_CIDNN = metrics.roc_auc_score(y_tgt_test, (predicted_LDR_CIDNN).reshape(-1))\n",
        "  auc_NLDR_CIDNN = metrics.roc_auc_score(y_tgt_test, (predicted_NLDR_CIDNN).reshape(-1))\n",
        "  auc_NLDR_NN_joint = metrics.roc_auc_score(y_tgt_test, (predicted_NLDR_NN_joint).reshape(-1))\n",
        "  auc_LDR_NN_joint = metrics.roc_auc_score(y_tgt_test, (predicted_LDR_NN_joint).reshape(-1))\n",
        "\n",
        "  acc_Baseline = np.mean(((predicted_Baseline > 0.5) *1).reshape(-1) == y_tgt_test)\n",
        "  acc_NN = np.mean(((predicted_NN > 0.5) *1).reshape(-1) == y_tgt_test)\n",
        "  acc_NN_joint = np.mean(((predicted_NN_joint > 0.5) *1).reshape(-1) == y_tgt_test)\n",
        "  acc_LDR_CIDNN = np.mean(((predicted_LDR_CIDNN > 0.5) *1).reshape(-1) == y_tgt_test)\n",
        "  acc_NLDR_CIDNN = np.mean(((predicted_NLDR_CIDNN > 0.5) *1).reshape(-1) == y_tgt_test)\n",
        "  acc_LDR_NN_joint = np.mean(((predicted_LDR_NN_joint > 0.5) *1).reshape(-1) == y_tgt_test)\n",
        "  acc_NLDR_NN_joint = np.mean(((predicted_NLDR_NN_joint > 0.5) *1).reshape(-1) == y_tgt_test)\n",
        "\n",
        "  f1_Baseline = metrics.f1_score(y_tgt_test, ((predicted_Baseline>0.5) *1).reshape(-1))\n",
        "  f1_NN = metrics.f1_score(y_tgt_test, ((predicted_NN>0.5) *1).reshape(-1))\n",
        "  f1_NN_joint = metrics.f1_score(y_tgt_test, ((predicted_NN_joint>0.5) *1).reshape(-1))\n",
        "  f1_LDR_CIDNN = metrics.f1_score(y_tgt_test, ((predicted_LDR_CIDNN>0.5)*1).reshape(-1))\n",
        "  f1_NLDR_CIDNN = metrics.f1_score(y_tgt_test, ((predicted_NLDR_CIDNN>0.5)*1).reshape(-1))\n",
        "  f1_LDR_NN_joint = metrics.f1_score(y_tgt_test, ((predicted_LDR_NN_joint>0.5)*1).reshape(-1))\n",
        "  f1_NLDR_NN_joint = metrics.f1_score(y_tgt_test, ((predicted_NLDR_NN_joint>0.5)*1).reshape(-1))\n",
        "\n",
        "  scores[fold_no] = np.array([auc_Baseline, auc_NN, auc_NN_joint, auc_LDR_NN_joint, auc_LDR_CIDNN, auc_NLDR_NN_joint, auc_NLDR_CIDNN,  \n",
        "                              acc_Baseline, acc_NN, acc_NN_joint, acc_LDR_NN_joint, acc_LDR_CIDNN, acc_NLDR_NN_joint, acc_NLDR_CIDNN,  \n",
        "                              f1_Baseline, f1_NN, f1_NN_joint, f1_LDR_NN_joint, f1_LDR_CIDNN, f1_NLDR_NN_joint, f1_NLDR_CIDNN])\n",
        "\n",
        "scores_final = np.vstack(scores.values())\n",
        "\n",
        "scores_final = pd.DataFrame(scores_final, columns = ['auc_Baseline','auc_NN', 'auc_NN_joint', 'auc_LDR_NN_joint', 'auc_LDR_CIDNN', 'auc_NLDR_NN_joint', 'auc_NLDR_CIDNN',\n",
        "                                                     'acc_Baseline','acc_NN', 'acc_NN_joint', 'acc_LDR_NN_joint', 'acc_LDR_CIDNN','acc_NLDR_NN_joint', 'acc_NLDR_CIDNN',\n",
        "                                                     'f1_Baseline','f1_NN', 'f1_NN_joint', 'f1_LDR_NN_joint','f1_LDR_CIDNN','f1_NLDR_NN_joint', 'f1_NLDR_CIDNN'])\n",
        "\n",
        "\n",
        "scores_final.to_csv('/content/drive/MyDrive/Can_a_joint_model_assist_target_label_prediction/FashionMNIST/scores_final.csv')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        },
        "id": "9LjjYm54jKrx",
        "outputId": "d22e2f8b-6105-4f5b-849b-84fdf84927c7"
      },
      "source": [
        "scores_final"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>auc_Baseline</th>\n",
              "      <th>auc_NN</th>\n",
              "      <th>auc_NN_joint</th>\n",
              "      <th>auc_LDR_NN_joint</th>\n",
              "      <th>auc_LDR_CIDNN</th>\n",
              "      <th>auc_NLDR_NN_joint</th>\n",
              "      <th>auc_NLDR_CIDNN</th>\n",
              "      <th>acc_Baseline</th>\n",
              "      <th>acc_NN</th>\n",
              "      <th>acc_NN_joint</th>\n",
              "      <th>acc_LDR_NN_joint</th>\n",
              "      <th>acc_LDR_CIDNN</th>\n",
              "      <th>acc_NLDR_NN_joint</th>\n",
              "      <th>acc_NLDR_CIDNN</th>\n",
              "      <th>f1_Baseline</th>\n",
              "      <th>f1_NN</th>\n",
              "      <th>f1_NN_joint</th>\n",
              "      <th>f1_LDR_NN_joint</th>\n",
              "      <th>f1_LDR_CIDNN</th>\n",
              "      <th>f1_NLDR_NN_joint</th>\n",
              "      <th>f1_NLDR_CIDNN</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.929552</td>\n",
              "      <td>0.966203</td>\n",
              "      <td>0.956252</td>\n",
              "      <td>0.959012</td>\n",
              "      <td>0.967696</td>\n",
              "      <td>0.967093</td>\n",
              "      <td>0.968144</td>\n",
              "      <td>0.929500</td>\n",
              "      <td>0.944167</td>\n",
              "      <td>0.935833</td>\n",
              "      <td>0.939833</td>\n",
              "      <td>0.944667</td>\n",
              "      <td>0.945167</td>\n",
              "      <td>0.946000</td>\n",
              "      <td>0.553326</td>\n",
              "      <td>0.685446</td>\n",
              "      <td>0.647113</td>\n",
              "      <td>0.656518</td>\n",
              "      <td>0.690299</td>\n",
              "      <td>0.702800</td>\n",
              "      <td>0.700555</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.930776</td>\n",
              "      <td>0.971275</td>\n",
              "      <td>0.965515</td>\n",
              "      <td>0.968203</td>\n",
              "      <td>0.968640</td>\n",
              "      <td>0.969553</td>\n",
              "      <td>0.968235</td>\n",
              "      <td>0.930000</td>\n",
              "      <td>0.952000</td>\n",
              "      <td>0.945667</td>\n",
              "      <td>0.948167</td>\n",
              "      <td>0.951500</td>\n",
              "      <td>0.947667</td>\n",
              "      <td>0.950833</td>\n",
              "      <td>0.567901</td>\n",
              "      <td>0.745583</td>\n",
              "      <td>0.714035</td>\n",
              "      <td>0.721076</td>\n",
              "      <td>0.744513</td>\n",
              "      <td>0.721631</td>\n",
              "      <td>0.742358</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.925284</td>\n",
              "      <td>0.967267</td>\n",
              "      <td>0.964523</td>\n",
              "      <td>0.965928</td>\n",
              "      <td>0.963767</td>\n",
              "      <td>0.968008</td>\n",
              "      <td>0.963916</td>\n",
              "      <td>0.926000</td>\n",
              "      <td>0.947000</td>\n",
              "      <td>0.946333</td>\n",
              "      <td>0.948667</td>\n",
              "      <td>0.945000</td>\n",
              "      <td>0.950167</td>\n",
              "      <td>0.944333</td>\n",
              "      <td>0.537500</td>\n",
              "      <td>0.717584</td>\n",
              "      <td>0.719023</td>\n",
              "      <td>0.725000</td>\n",
              "      <td>0.700544</td>\n",
              "      <td>0.734222</td>\n",
              "      <td>0.702847</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.927854</td>\n",
              "      <td>0.965321</td>\n",
              "      <td>0.969731</td>\n",
              "      <td>0.969901</td>\n",
              "      <td>0.972027</td>\n",
              "      <td>0.970899</td>\n",
              "      <td>0.972726</td>\n",
              "      <td>0.924500</td>\n",
              "      <td>0.946667</td>\n",
              "      <td>0.947667</td>\n",
              "      <td>0.948500</td>\n",
              "      <td>0.952833</td>\n",
              "      <td>0.949667</td>\n",
              "      <td>0.952333</td>\n",
              "      <td>0.511327</td>\n",
              "      <td>0.706422</td>\n",
              "      <td>0.716094</td>\n",
              "      <td>0.718323</td>\n",
              "      <td>0.744354</td>\n",
              "      <td>0.726449</td>\n",
              "      <td>0.744186</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.919498</td>\n",
              "      <td>0.965731</td>\n",
              "      <td>0.948440</td>\n",
              "      <td>0.952932</td>\n",
              "      <td>0.961681</td>\n",
              "      <td>0.959611</td>\n",
              "      <td>0.961012</td>\n",
              "      <td>0.922500</td>\n",
              "      <td>0.947667</td>\n",
              "      <td>0.931500</td>\n",
              "      <td>0.936333</td>\n",
              "      <td>0.942833</td>\n",
              "      <td>0.939500</td>\n",
              "      <td>0.944167</td>\n",
              "      <td>0.515120</td>\n",
              "      <td>0.720641</td>\n",
              "      <td>0.641674</td>\n",
              "      <td>0.647601</td>\n",
              "      <td>0.686758</td>\n",
              "      <td>0.669700</td>\n",
              "      <td>0.694622</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.925954</td>\n",
              "      <td>0.970017</td>\n",
              "      <td>0.961110</td>\n",
              "      <td>0.962196</td>\n",
              "      <td>0.964220</td>\n",
              "      <td>0.963400</td>\n",
              "      <td>0.964610</td>\n",
              "      <td>0.922833</td>\n",
              "      <td>0.947000</td>\n",
              "      <td>0.940667</td>\n",
              "      <td>0.940667</td>\n",
              "      <td>0.944000</td>\n",
              "      <td>0.943667</td>\n",
              "      <td>0.943333</td>\n",
              "      <td>0.539303</td>\n",
              "      <td>0.722028</td>\n",
              "      <td>0.702341</td>\n",
              "      <td>0.685512</td>\n",
              "      <td>0.703704</td>\n",
              "      <td>0.702988</td>\n",
              "      <td>0.702797</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.926843</td>\n",
              "      <td>0.956773</td>\n",
              "      <td>0.964652</td>\n",
              "      <td>0.964282</td>\n",
              "      <td>0.967898</td>\n",
              "      <td>0.965658</td>\n",
              "      <td>0.968652</td>\n",
              "      <td>0.928833</td>\n",
              "      <td>0.939333</td>\n",
              "      <td>0.947667</td>\n",
              "      <td>0.947000</td>\n",
              "      <td>0.950500</td>\n",
              "      <td>0.948500</td>\n",
              "      <td>0.950167</td>\n",
              "      <td>0.545261</td>\n",
              "      <td>0.653333</td>\n",
              "      <td>0.709797</td>\n",
              "      <td>0.697719</td>\n",
              "      <td>0.721127</td>\n",
              "      <td>0.710945</td>\n",
              "      <td>0.728921</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.927053</td>\n",
              "      <td>0.967252</td>\n",
              "      <td>0.949013</td>\n",
              "      <td>0.955925</td>\n",
              "      <td>0.967061</td>\n",
              "      <td>0.956331</td>\n",
              "      <td>0.966829</td>\n",
              "      <td>0.925333</td>\n",
              "      <td>0.951500</td>\n",
              "      <td>0.941000</td>\n",
              "      <td>0.946167</td>\n",
              "      <td>0.951500</td>\n",
              "      <td>0.944333</td>\n",
              "      <td>0.950667</td>\n",
              "      <td>0.535270</td>\n",
              "      <td>0.727273</td>\n",
              "      <td>0.667917</td>\n",
              "      <td>0.690316</td>\n",
              "      <td>0.723647</td>\n",
              "      <td>0.689013</td>\n",
              "      <td>0.722326</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.927444</td>\n",
              "      <td>0.963757</td>\n",
              "      <td>0.970796</td>\n",
              "      <td>0.970722</td>\n",
              "      <td>0.972968</td>\n",
              "      <td>0.972671</td>\n",
              "      <td>0.972666</td>\n",
              "      <td>0.928333</td>\n",
              "      <td>0.942833</td>\n",
              "      <td>0.948333</td>\n",
              "      <td>0.948667</td>\n",
              "      <td>0.950500</td>\n",
              "      <td>0.949333</td>\n",
              "      <td>0.948833</td>\n",
              "      <td>0.570858</td>\n",
              "      <td>0.708085</td>\n",
              "      <td>0.739496</td>\n",
              "      <td>0.735395</td>\n",
              "      <td>0.745064</td>\n",
              "      <td>0.741497</td>\n",
              "      <td>0.738278</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.920852</td>\n",
              "      <td>0.962665</td>\n",
              "      <td>0.957976</td>\n",
              "      <td>0.961897</td>\n",
              "      <td>0.969732</td>\n",
              "      <td>0.965674</td>\n",
              "      <td>0.969092</td>\n",
              "      <td>0.925667</td>\n",
              "      <td>0.944167</td>\n",
              "      <td>0.938167</td>\n",
              "      <td>0.943833</td>\n",
              "      <td>0.948500</td>\n",
              "      <td>0.945333</td>\n",
              "      <td>0.948167</td>\n",
              "      <td>0.505543</td>\n",
              "      <td>0.675073</td>\n",
              "      <td>0.648341</td>\n",
              "      <td>0.668633</td>\n",
              "      <td>0.707109</td>\n",
              "      <td>0.682171</td>\n",
              "      <td>0.704091</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   auc_Baseline    auc_NN  ...  f1_NLDR_NN_joint  f1_NLDR_CIDNN\n",
              "0      0.929552  0.966203  ...          0.702800       0.700555\n",
              "1      0.930776  0.971275  ...          0.721631       0.742358\n",
              "2      0.925284  0.967267  ...          0.734222       0.702847\n",
              "3      0.927854  0.965321  ...          0.726449       0.744186\n",
              "4      0.919498  0.965731  ...          0.669700       0.694622\n",
              "5      0.925954  0.970017  ...          0.702988       0.702797\n",
              "6      0.926843  0.956773  ...          0.710945       0.728921\n",
              "7      0.927053  0.967252  ...          0.689013       0.722326\n",
              "8      0.927444  0.963757  ...          0.741497       0.738278\n",
              "9      0.920852  0.962665  ...          0.682171       0.704091\n",
              "\n",
              "[10 rows x 21 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4AhNzY8jUnd",
        "outputId": "23507362-5df6-4c2b-c3df-110ca5ad34bb"
      },
      "source": [
        "np.mean(scores_final)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "auc_Baseline         0.926111\n",
              "auc_NN               0.965626\n",
              "auc_NN_joint         0.960801\n",
              "auc_LDR_NN_joint     0.963100\n",
              "auc_LDR_CIDNN        0.967569\n",
              "auc_NLDR_NN_joint    0.965890\n",
              "auc_NLDR_CIDNN       0.967588\n",
              "acc_Baseline         0.926350\n",
              "acc_NN               0.946233\n",
              "acc_NN_joint         0.942283\n",
              "acc_LDR_NN_joint     0.944783\n",
              "acc_LDR_CIDNN        0.948183\n",
              "acc_NLDR_NN_joint    0.946333\n",
              "acc_NLDR_CIDNN       0.947883\n",
              "f1_Baseline          0.538141\n",
              "f1_NN                0.706147\n",
              "f1_NN_joint          0.690583\n",
              "f1_LDR_NN_joint      0.694609\n",
              "f1_LDR_CIDNN         0.716712\n",
              "f1_NLDR_NN_joint     0.708142\n",
              "f1_NLDR_CIDNN        0.718098\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    }
  ]
}